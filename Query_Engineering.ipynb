{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b64b926-d53b-4fd2-975f-ef3f164406f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770b44f-038e-4b76-81c2-09940289685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 2025 Document Retrieval System\n",
    "\n",
    "\n",
    "# Import the API key from config.py\n",
    "try:\n",
    "    from config import OPENAI_API_KEY\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please create a config.py file with your OPENAI_API_KEY\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Connect to the existing database\n",
    "conn = sqlite3.connect('p2025_db.sqlite')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def transform_query(query, verbose=False):\n",
    "    system_message = \"\"\"You are an AI assistant helping to optimize queries for semantic search. Your task is to rewrite the given query in a way that will improve its semantic matching with relevant document chunks.\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"Hey, I have a query: \"{query}\", and a 900 page document called Project 2025, which is a sweeping, 900-page plan drummed up by a conservative think tank targeting the executive branch and laying out right-wing priorities for everything from America's education system to the border and abortion restrictions. The document is chunked out and vectorized using OpenAI embeddings. How might I rewrite the query such that when it gets vectorized, its nearest neighbors will contain the answers to the question?\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "        transformed_query = response.choices[0].message.content.strip()\n",
    "        if verbose:\n",
    "            print(f\"Original query: {query}\")\n",
    "            print(f\"Transformed query: {transformed_query}\")\n",
    "        return transformed_query\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in transforming query: {e}\")\n",
    "        return query  # Return original query if transformation fails\n",
    "\n",
    "def encode_text(text, max_retries=10, backoff_factor=2, timeout=30, verbose=False):\n",
    "    if verbose:\n",
    "        print(f\"Starting to encode text of length {len(text)}\")\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if verbose:\n",
    "                print(f\"Attempt {attempt + 1} to encode text\")\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=[text],\n",
    "                timeout=timeout\n",
    "            )\n",
    "            embedding = np.array(response.data[0].embedding)\n",
    "            if verbose:\n",
    "                print(f\"Successfully encoded text\")\n",
    "            return embedding, embedding.shape\n",
    "        except Exception as e:\n",
    "            wait_time = backoff_factor * (2 ** attempt)\n",
    "            if verbose:\n",
    "                print(f\"Error occurred: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    if verbose:\n",
    "        print(\"Failed to encode text after all attempts\")\n",
    "    raise Exception(\"Failed to encode text after all attempts\")\n",
    "\n",
    "def retrieve_chunks(query, top_k=20, verbose=False):\n",
    "    transformed_query = transform_query(query, verbose)\n",
    "    if verbose:\n",
    "        print(f\"Retrieving chunks for transformed query: '{transformed_query}'\")\n",
    "    query_embedding, query_shape = encode_text(transformed_query, verbose=verbose)\n",
    "    if verbose:\n",
    "        print(f\"Query embedding shape: {query_shape}\")\n",
    "    \n",
    "    cursor.execute('SELECT id, embedding, shape FROM document_chunks')\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Comparing query to {len(results)} stored chunks\")\n",
    "    similarities = []\n",
    "    for id, emb, shape in (tqdm(results, desc=\"Comparing embeddings\") if verbose else results):\n",
    "        emb_array = np.array([float(x) for x in emb.split(',')]).reshape(eval(shape))\n",
    "        \n",
    "        if emb_array.shape != query_shape:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Embedding shape mismatch. Query: {query_shape}, Stored: {emb_array.shape}\")\n",
    "            continue\n",
    "        \n",
    "        similarity = np.dot(query_embedding, emb_array) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb_array))\n",
    "        similarities.append((id, similarity))\n",
    "    \n",
    "    if not similarities:\n",
    "        if verbose:\n",
    "            print(\"No valid embeddings found for comparison.\")\n",
    "        return []\n",
    "    \n",
    "    top_ids = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    placeholders = ','.join('?' for _ in top_ids)\n",
    "    cursor.execute(f'SELECT content FROM document_chunks WHERE id IN ({placeholders})', \n",
    "                   [id for id, _ in top_ids])\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def select_promising_chunks(query, chunks, verbose=False):\n",
    "    system_message = \"\"\"You are an AI assistant helping to identify relevant text chunks for a given query. Your task is to select the most promising chunks that are likely to contain information to answer the query. Provide your reasoning, then list the selected chunk numbers in a specific format.\"\"\"\n",
    "\n",
    "    chunk_previews = \"\\n\".join([f\"Chunk {i+1}: {chunk[0][:200]}...\" for i, chunk in enumerate(chunks)])\n",
    "    \n",
    "    user_message = f\"\"\"Query: \"{query}\"\n",
    "\n",
    "Here are previews of several text chunks:\n",
    "\n",
    "{chunk_previews}\n",
    "\n",
    "Please identify the chunks that seem most relevant to answering the query. Explain your reasoning briefly, then list only the numbers of the selected chunks.\n",
    "\n",
    "End your response with the comma-separated chunk numbers enclosed in curly brackets, like this: {{1,4,7}}. Include only the most relevant chunks, aiming for 3-5 selections unless more are clearly relevant.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "        full_response = response.choices[0].message.content.strip()\n",
    "        if verbose:\n",
    "            print(f\"GPT-4 response:\\n{full_response}\")\n",
    "        \n",
    "        match = re.search(r'\\{([^}]+)\\}$', full_response)\n",
    "        if match:\n",
    "            chunk_numbers = match.group(1)\n",
    "            selected_chunks = [int(num.strip()) for num in chunk_numbers.split(',') if num.strip().isdigit()]\n",
    "            if verbose:\n",
    "                print(f\"Selected chunks: {selected_chunks}\")\n",
    "            return selected_chunks\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"No properly formatted chunk numbers found. Defaulting to first 5 chunks.\")\n",
    "            return list(range(1, min(6, len(chunks)+1)))\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in selecting promising chunks: {e}\")\n",
    "        return list(range(1, min(6, len(chunks)+1)))  # Default to first 5 chunks if there's an error\n",
    "\n",
    "def print_chunks(chunks, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"\\nRelevant chunks for the query:\")\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"Chunk {i}:\")\n",
    "            print(chunk[0][:200] + \"...\")  # Print first 200 characters of each chunk\n",
    "            print()\n",
    "\n",
    "def generate_final_answer(original_query, relevant_chunks, max_chunk_length=500, verbose=False):\n",
    "    combined_context = \" \".join([chunk[0][:max_chunk_length] for chunk in relevant_chunks])\n",
    "    \n",
    "    system_message = \"\"\"You are an AI assistant tasked with answering questions about Project 2025 based on provided context. Use the given information to answer the question accurately and concisely.\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"Context from Project 2025 document:\n",
    "\n",
    "{combined_context}\n",
    "\n",
    "Based on this context, please answer the following question:\n",
    "{original_query}\n",
    "\n",
    "Provide a concise answer that directly addresses the question using only the information given in the context.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "        final_answer = response.choices[0].message.content.strip()\n",
    "        return final_answer\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in generating final answer: {e}\")\n",
    "        return \"Unable to generate a final answer due to an error.\"\n",
    "\n",
    "def process_query(query, verbose=False):\n",
    "    print(f\"Query: {query}\")\n",
    "    all_chunks = retrieve_chunks(query, top_k=20, verbose=verbose)\n",
    "    selected_indices = select_promising_chunks(query, all_chunks, verbose=verbose)\n",
    "    relevant_chunks = [all_chunks[i-1] for i in selected_indices]  # -1 because chunk numbering starts at 1\n",
    "    \n",
    "    print_chunks(relevant_chunks, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nGenerating final answer...\")\n",
    "    final_answer = generate_final_answer(query, relevant_chunks, verbose=verbose)\n",
    "    print(\"\\nFinal Answer:\")\n",
    "    print(final_answer)\n",
    "\n",
    "# # Example usage\n",
    "# query = \"What does Project 2025 say about the consolidation of power into the president's seat?\"\n",
    "# process_query(query, verbose=True)\n",
    "\n",
    "# # Close the database connection when done\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75935a-9cb5-4ba3-b67c-a7aa2cb5e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What can be said about Bureau of Land Management?\"\n",
    "process_query(query, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe68fb-812e-4d7e-8913-a92142e82725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
