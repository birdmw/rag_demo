{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7fe7e-c142-4f3e-ba6e-ce6ce7c0e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "\n",
    "# Import the API key and org ID from config.py\n",
    "try:\n",
    "    from config import OPENAI_API_KEY\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please create a config.py file with your OPENAI_API_KEY and OPENAI_ORG_ID\")\n",
    "\n",
    "print(\"Setting up OpenAI client...\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"Initializing database...\")\n",
    "conn = sqlite3.connect('p2025_db.sqlite')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"Creating table if not exists...\")\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS document_chunks\n",
    "(id INTEGER PRIMARY KEY, content TEXT, embedding BLOB, shape TEXT)\n",
    "''')\n",
    "\n",
    "def encode_text(text, max_retries=10, backoff_factor=2, timeout=30):\n",
    "    print(f\"Starting to encode text of length {len(text)}\")\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} to encode text\")\n",
    "            response = client.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=[text],\n",
    "                timeout=timeout\n",
    "            )\n",
    "            embedding = np.array(response.data[0].embedding)\n",
    "            print(f\"Successfully encoded text\")\n",
    "            return embedding, embedding.shape\n",
    "        except Timeout:\n",
    "            wait_time = backoff_factor * (2 ** attempt)\n",
    "            print(f\"Request timed out. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "        except Exception as e:\n",
    "            wait_time = backoff_factor * (2 ** attempt)\n",
    "            print(f\"Error occurred: {e}. Retrying in {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "    print(\"Failed to encode text after all attempts\")\n",
    "    raise Exception(\"Failed to encode text after all attempts\")\n",
    "\n",
    "def add_chunk(content, embedding, shape):\n",
    "    cursor.execute('INSERT INTO document_chunks (content, embedding, shape) VALUES (?, ?, ?)',\n",
    "                   (content, ','.join(map(str, embedding)), str(shape)))\n",
    "    conn.commit()\n",
    "\n",
    "def read_and_chunk_file(file_path, chunk_size=3500, overlap=500):\n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    chunks = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    print(f\"Chunking file (chunk size: {chunk_size}, overlap: {overlap})\")\n",
    "    start = 0\n",
    "    with tqdm(total=len(content), desc=\"Chunking progress\") as pbar:\n",
    "        while start < len(content):\n",
    "            end = start + chunk_size\n",
    "            chunk = content[start:end]\n",
    "            \n",
    "            if end < len(content):\n",
    "                sentence_end = chunk.rfind('.')\n",
    "                paragraph_end = chunk.rfind('\\n')\n",
    "                if sentence_end > 0:\n",
    "                    end = start + sentence_end + 1\n",
    "                elif paragraph_end > 0:\n",
    "                    end = start + paragraph_end + 1\n",
    "            \n",
    "            chunks.append(content[start:end])\n",
    "            start = end - overlap\n",
    "            pbar.update(end - start)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "print(\"Reading and chunking file...\")\n",
    "chunks = read_and_chunk_file('p2025.txt')\n",
    "\n",
    "print(\"Processing chunks and adding to database...\")\n",
    "for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "    print(f\"Encoding chunk (length: {len(chunk)})\")\n",
    "    embedding, shape = encode_text(chunk)\n",
    "    print(f\"Adding chunk to database (embedding shape: {shape})\")\n",
    "    add_chunk(chunk, embedding, shape)\n",
    "\n",
    "print(f\"Added {len(chunks)} chunks to the database.\")\n",
    "\n",
    "def retrieve_chunks(query, top_k=5):\n",
    "    print(f\"Retrieving chunks for query: '{query}'\")\n",
    "    query_embedding, query_shape = encode_text(query)\n",
    "    print(f\"Query embedding shape: {query_shape}\")\n",
    "    \n",
    "    cursor.execute('SELECT id, embedding, shape FROM document_chunks')\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Comparing query to {len(results)} stored chunks\")\n",
    "    similarities = []\n",
    "    for id, emb, shape in tqdm(results, desc=\"Comparing embeddings\"):\n",
    "        emb_array = np.array([float(x) for x in emb.split(',')]).reshape(eval(shape))\n",
    "        \n",
    "        if emb_array.shape != query_shape:\n",
    "            print(f\"Warning: Embedding shape mismatch. Query: {query_shape}, Stored: {emb_array.shape}\")\n",
    "            continue\n",
    "        \n",
    "        similarity = np.dot(query_embedding, emb_array) / (np.linalg.norm(query_embedding) * np.linalg.norm(emb_array))\n",
    "        similarities.append((id, similarity))\n",
    "    \n",
    "    if not similarities:\n",
    "        print(\"No valid embeddings found for comparison.\")\n",
    "        return []\n",
    "    \n",
    "    top_ids = sorted(similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    placeholders = ','.join('?' for _ in top_ids)\n",
    "    cursor.execute(f'SELECT content FROM document_chunks WHERE id IN ({placeholders})', \n",
    "                   [id for id, _ in top_ids])\n",
    "    return cursor.fetchall()\n",
    "\n",
    "print(\"Testing retrieval...\")\n",
    "test_query = \"What does Project 2025 say about BLM's move west?\"\n",
    "relevant_chunks = retrieve_chunks(test_query)\n",
    "\n",
    "print(\"\\nRelevant chunks for the query:\")\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk[0][:200] + \"...\")  # Print first 200 characters of each chunk\n",
    "    print()\n",
    "\n",
    "print(\"Closing database connection...\")\n",
    "conn.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28420a-8d7a-44f7-b327-ab3a15fe7f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
